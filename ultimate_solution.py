"""
SC6117 åŠ å¯†è´§å¸é¢„æµ‹æ¯”èµ› - ç»ˆæè§£å†³æ–¹æ¡ˆ
Ultimate Ensemble Solution for Crypto Forecast Competition

æ ¸å¿ƒç­–ç•¥:
1. è¶…å¼ºç‰¹å¾å·¥ç¨‹ (200+ Alphaå› å­)
2. å¤šæ¨¡å‹å †å é›†æˆ (LightGBM + XGBoost + CatBoost + Neural Network)
3. è´å¶æ–¯ä¼˜åŒ–è¶…å‚æ•°
4. æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
5. åå¤„ç†ä¼˜åŒ–

ä½œè€…: AI Champion
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# åŸºç¡€åº“
from pathlib import Path
from typing import List, Tuple, Dict, Optional
import os
import random
from datetime import datetime

# æœºå™¨å­¦ä¹ 
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import TimeSeriesSplit
from scipy import stats
from scipy.signal import savgol_filter

# GBDTæ¨¡å‹
import lightgbm as lgb
from lightgbm import LGBMRegressor
import xgboost as xgb
from xgboost import XGBRegressor

# è®¾ç½®éšæœºç§å­
def set_seed(seed: int = 42):
    """è®¾ç½®å…¨å±€éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§"""
    np.random.seed(seed)
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)

set_seed(42)

print("=" * 80)
print("SC6117 åŠ å¯†è´§å¸é¢„æµ‹æ¯”èµ› - ç»ˆæè§£å†³æ–¹æ¡ˆ")
print("=" * 80)


# ============================================
# ç¬¬ä¸€éƒ¨åˆ†: æ•°æ®åŠ è½½
# ============================================
print("\nğŸ“Š åŠ è½½æ•°æ®...")

train_df = pd.read_csv('data/train.csv')
test_df = pd.read_csv('data/test.csv')

train_df['Timestamp'] = pd.to_datetime(train_df['Timestamp'])
test_df['Timestamp'] = pd.to_datetime(test_df['Timestamp'])

train_df = train_df.sort_values('Timestamp').reset_index(drop=True)
test_df = test_df.sort_values('Timestamp').reset_index(drop=True)

print(f"è®­ç»ƒé›†: {train_df.shape[0]:,} æ ·æœ¬, {train_df.shape[1]} åˆ—")
print(f"æµ‹è¯•é›†: {test_df.shape[0]:,} æ ·æœ¬, {test_df.shape[1]} åˆ—")
print(f"è®­ç»ƒé›†æ—¶é—´: {train_df['Timestamp'].min()} åˆ° {train_df['Timestamp'].max()}")
print(f"æµ‹è¯•é›†æ—¶é—´: {test_df['Timestamp'].min()} åˆ° {test_df['Timestamp'].max()}")


# ============================================
# ç¬¬äºŒéƒ¨åˆ†: ç»ˆæç‰¹å¾å·¥ç¨‹
# ============================================
print("\nğŸ”§ å¼€å§‹ç»ˆæç‰¹å¾å·¥ç¨‹...")

def calculate_rsi(series: pd.Series, period: int = 14) -> pd.Series:
    """è®¡ç®—RSI"""
    delta = series.diff()
    gain = delta.where(delta > 0, 0).rolling(window=period).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
    rs = gain / (loss + 1e-10)
    return 100 - (100 / (1 + rs))

def calculate_macd(series: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9):
    """è®¡ç®—MACD"""
    exp_fast = series.ewm(span=fast, adjust=False).mean()
    exp_slow = series.ewm(span=slow, adjust=False).mean()
    macd = exp_fast - exp_slow
    macd_signal = macd.ewm(span=signal, adjust=False).mean()
    macd_hist = macd - macd_signal
    return macd, macd_signal, macd_hist

def calculate_bollinger_bands(series: pd.Series, period: int = 20, std_dev: float = 2.0):
    """è®¡ç®—å¸ƒæ—å¸¦"""
    sma = series.rolling(window=period).mean()
    std = series.rolling(window=period).std()
    upper = sma + std_dev * std
    lower = sma - std_dev * std
    width = (upper - lower) / (sma + 1e-10)
    position = (series - lower) / (upper - lower + 1e-10)
    return upper, lower, width, position

def calculate_atr(high: pd.Series, low: pd.Series, close: pd.Series, period: int = 14) -> pd.Series:
    """è®¡ç®—ATR"""
    prev_close = close.shift(1)
    tr = pd.concat([
        high - low,
        (high - prev_close).abs(),
        (low - prev_close).abs()
    ], axis=1).max(axis=1)
    return tr.rolling(window=period).mean()

def calculate_stochastic(high: pd.Series, low: pd.Series, close: pd.Series, 
                         k_period: int = 14, d_period: int = 3):
    """è®¡ç®—éšæœºæŒ‡æ ‡"""
    lowest_low = low.rolling(window=k_period).min()
    highest_high = high.rolling(window=k_period).max()
    k = 100 * (close - lowest_low) / (highest_high - lowest_low + 1e-10)
    d = k.rolling(window=d_period).mean()
    return k, d

def calculate_williams_r(high: pd.Series, low: pd.Series, close: pd.Series, period: int = 14):
    """è®¡ç®—å¨å»‰å§†æ–¯%R"""
    highest_high = high.rolling(window=period).max()
    lowest_low = low.rolling(window=period).min()
    return -100 * (highest_high - close) / (highest_high - lowest_low + 1e-10)

def calculate_cci(high: pd.Series, low: pd.Series, close: pd.Series, period: int = 20):
    """è®¡ç®—å•†å“é€šé“æŒ‡æ•°"""
    tp = (high + low + close) / 3
    sma = tp.rolling(window=period).mean()
    mad = tp.rolling(window=period).apply(lambda x: np.abs(x - x.mean()).mean())
    return (tp - sma) / (0.015 * mad + 1e-10)

def calculate_obv(close: pd.Series, volume: pd.Series) -> pd.Series:
    """è®¡ç®—èƒ½é‡æ½®"""
    return (np.sign(close.diff()) * volume).cumsum()

def calculate_mfi(high: pd.Series, low: pd.Series, close: pd.Series, 
                  volume: pd.Series, period: int = 14) -> pd.Series:
    """è®¡ç®—èµ„é‡‘æµé‡æŒ‡æ•°"""
    typical_price = (high + low + close) / 3
    money_flow = typical_price * volume
    delta = typical_price.diff()
    
    positive_flow = money_flow.where(delta > 0, 0).rolling(window=period).sum()
    negative_flow = money_flow.where(delta < 0, 0).rolling(window=period).sum()
    
    money_ratio = positive_flow / (negative_flow + 1e-10)
    return 100 - (100 / (1 + money_ratio))

def calculate_adx(high: pd.Series, low: pd.Series, close: pd.Series, period: int = 14):
    """è®¡ç®—ADX"""
    plus_dm = high.diff()
    minus_dm = -low.diff()
    
    plus_dm = plus_dm.where((plus_dm > minus_dm) & (plus_dm > 0), 0)
    minus_dm = minus_dm.where((minus_dm > plus_dm) & (minus_dm > 0), 0)
    
    tr = calculate_atr(high, low, close, 1)
    
    plus_di = 100 * (plus_dm.ewm(span=period).mean() / (tr.ewm(span=period).mean() + 1e-10))
    minus_di = 100 * (minus_dm.ewm(span=period).mean() / (tr.ewm(span=period).mean() + 1e-10))
    
    dx = 100 * np.abs(plus_di - minus_di) / (plus_di + minus_di + 1e-10)
    adx = dx.ewm(span=period).mean()
    
    return adx, plus_di, minus_di


def create_ultimate_features(df: pd.DataFrame, is_train: bool = True) -> pd.DataFrame:
    """
    ç»ˆæç‰¹å¾å·¥ç¨‹ - åˆ›å»º200+ä¸ªé«˜è´¨é‡Alphaå› å­
    
    æ—¶é—´å‘¨æœŸè¯´æ˜ (15åˆ†é’Ÿé—´éš”):
    - 4 = 1å°æ—¶
    - 24 = 6å°æ—¶  
    - 96 = 1å¤©
    - 672 = 1å‘¨
    - 2880 = 1ä¸ªæœˆ
    """
    df = df.copy()
    
    # =============================================
    # 1. æ—¶é—´ç‰¹å¾ (Time Features)
    # =============================================
    df['hour'] = df['Timestamp'].dt.hour
    df['day_of_week'] = df['Timestamp'].dt.dayofweek
    df['day_of_month'] = df['Timestamp'].dt.day
    df['month'] = df['Timestamp'].dt.month
    df['quarter'] = df['Timestamp'].dt.quarter
    df['year'] = df['Timestamp'].dt.year
    df['week_of_year'] = df['Timestamp'].dt.isocalendar().week.astype(int)
    df['day_of_year'] = df['Timestamp'].dt.dayofyear
    
    # å‘¨æœŸæ€§ç¼–ç  (Cyclical Encoding)
    for col, period in [('hour', 24), ('day_of_week', 7), ('month', 12), 
                        ('day_of_month', 31), ('week_of_year', 52)]:
        df[f'{col}_sin'] = np.sin(2 * np.pi * df[col] / period)
        df[f'{col}_cos'] = np.cos(2 * np.pi * df[col] / period)
    
    # äº¤æ˜“æ—¶æ®µ
    df['is_asia_session'] = ((df['hour'] >= 0) & (df['hour'] < 8)).astype(int)
    df['is_europe_session'] = ((df['hour'] >= 8) & (df['hour'] < 16)).astype(int)
    df['is_us_session'] = ((df['hour'] >= 16) & (df['hour'] < 24)).astype(int)
    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
    
    # =============================================
    # 2. ä»·æ ¼åŸºç¡€ç‰¹å¾ (Price Features)
    # =============================================
    # æ”¶ç›Šç‡
    df['return_1'] = df['Close'].pct_change(1)
    for lag in [2, 3, 4, 6, 8, 12, 24, 48, 96, 192, 384]:
        df[f'return_{lag}'] = df['Close'].pct_change(lag)
    
    # å¯¹æ•°æ”¶ç›Šç‡ (æ›´ç¨³å®š)
    df['log_return_1'] = np.log(df['Close'] / df['Close'].shift(1))
    for lag in [4, 12, 24, 48, 96]:
        df[f'log_return_{lag}'] = np.log(df['Close'] / df['Close'].shift(lag))
    
    # ä»·æ ¼èŒƒå›´å’Œèœ¡çƒ›å›¾ç‰¹å¾
    df['price_range'] = df['High'] - df['Low']
    df['price_change'] = df['Close'] - df['Open']
    df['price_change_pct'] = df['price_change'] / (df['Open'] + 1e-10)
    
    # OHLCæ¯”ç‡
    df['high_low_ratio'] = df['High'] / (df['Low'] + 1e-10)
    df['close_open_ratio'] = df['Close'] / (df['Open'] + 1e-10)
    df['high_close_ratio'] = df['High'] / (df['Close'] + 1e-10)
    df['low_close_ratio'] = df['Low'] / (df['Close'] + 1e-10)
    df['high_open_ratio'] = df['High'] / (df['Open'] + 1e-10)
    df['low_open_ratio'] = df['Low'] / (df['Open'] + 1e-10)
    
    # èœ¡çƒ›å›¾å½¢æ€
    df['upper_shadow'] = df['High'] - df[['Open', 'Close']].max(axis=1)
    df['lower_shadow'] = df[['Open', 'Close']].min(axis=1) - df['Low']
    df['body_size'] = (df['Close'] - df['Open']).abs()
    df['body_direction'] = np.sign(df['Close'] - df['Open'])
    df['body_to_range'] = df['body_size'] / (df['price_range'] + 1e-10)
    df['upper_shadow_ratio'] = df['upper_shadow'] / (df['price_range'] + 1e-10)
    df['lower_shadow_ratio'] = df['lower_shadow'] / (df['price_range'] + 1e-10)
    
    # å…¸å‹ä»·æ ¼
    df['typical_price'] = (df['High'] + df['Low'] + df['Close']) / 3
    df['weighted_close'] = (df['High'] + df['Low'] + 2 * df['Close']) / 4
    
    # =============================================
    # 3. æˆäº¤é‡ç‰¹å¾ (Volume Features)
    # =============================================
    df['volume_log'] = np.log1p(df['Volume'])
    df['volume_change'] = df['Volume'].pct_change(1)
    
    # æˆäº¤é‡ç§»åŠ¨å¹³å‡
    for window in [4, 12, 24, 48, 96]:
        df[f'volume_ma_{window}'] = df['Volume'].shift(1).rolling(window=window).mean()
        df[f'volume_std_{window}'] = df['Volume'].shift(1).rolling(window=window).std()
        df[f'volume_ratio_{window}'] = df['Volume'] / (df[f'volume_ma_{window}'] + 1e-10)
    
    # ä»·é‡å…³ç³»
    df['price_volume'] = df['price_change'] * df['Volume']
    df['price_volume_log'] = df['price_change'] * df['volume_log']
    df['return_volume'] = df['return_1'] * df['Volume']
    
    # ä»·é‡ç›¸å…³æ€§
    for window in [12, 24, 48]:
        df[f'price_volume_corr_{window}'] = df['return_1'].rolling(window=window).corr(df['Volume'])
    
    # OBV
    df['OBV'] = calculate_obv(df['Close'], df['Volume'])
    df['OBV_ma_12'] = df['OBV'].rolling(window=12).mean()
    df['OBV_ratio'] = df['OBV'] / (df['OBV_ma_12'] + 1e-10)
    
    # =============================================
    # 4. æŠ€æœ¯æŒ‡æ ‡ (Technical Indicators)
    # =============================================
    # RSI - å¤šå‘¨æœŸ
    for period in [6, 9, 14, 21, 28]:
        df[f'RSI_{period}'] = calculate_rsi(df['Close'], period)
    
    # RSIå˜åŒ–ç‡
    df['RSI_14_change'] = df['RSI_14'].diff(1)
    df['RSI_14_ma'] = df['RSI_14'].rolling(window=12).mean()
    
    # MACD
    macd, macd_signal, macd_hist = calculate_macd(df['Close'])
    df['MACD'] = macd
    df['MACD_signal'] = macd_signal
    df['MACD_hist'] = macd_hist
    df['MACD_hist_change'] = df['MACD_hist'].diff(1)
    
    # å¸ƒæ—å¸¦
    for period in [10, 20, 40]:
        upper, lower, width, position = calculate_bollinger_bands(df['Close'], period)
        df[f'BB_upper_{period}'] = upper
        df[f'BB_lower_{period}'] = lower
        df[f'BB_width_{period}'] = width
        df[f'BB_position_{period}'] = position
    
    # ATR
    for period in [7, 14, 21, 28]:
        df[f'ATR_{period}'] = calculate_atr(df['High'], df['Low'], df['Close'], period)
        df[f'ATR_ratio_{period}'] = df[f'ATR_{period}'] / (df['Close'] + 1e-10)
    
    # éšæœºæŒ‡æ ‡ (Stochastic)
    k, d = calculate_stochastic(df['High'], df['Low'], df['Close'], 14, 3)
    df['Stoch_K'] = k
    df['Stoch_D'] = d
    df['Stoch_diff'] = k - d
    
    # å¨å»‰å§†æ–¯%R
    df['Williams_R'] = calculate_williams_r(df['High'], df['Low'], df['Close'], 14)
    
    # CCI
    for period in [14, 20]:
        df[f'CCI_{period}'] = calculate_cci(df['High'], df['Low'], df['Close'], period)
    
    # MFI
    df['MFI'] = calculate_mfi(df['High'], df['Low'], df['Close'], df['Volume'], 14)
    
    # ADX
    adx, plus_di, minus_di = calculate_adx(df['High'], df['Low'], df['Close'], 14)
    df['ADX'] = adx
    df['Plus_DI'] = plus_di
    df['Minus_DI'] = minus_di
    df['DI_diff'] = plus_di - minus_di
    
    # =============================================
    # 5. ç§»åŠ¨å¹³å‡å’Œè¶‹åŠ¿ç‰¹å¾ (Trend Features)
    # =============================================
    # SMA
    for window in [4, 8, 12, 24, 48, 96, 192]:
        df[f'SMA_{window}'] = df['Close'].shift(1).rolling(window=window).mean()
        df[f'close_SMA_ratio_{window}'] = df['Close'] / (df[f'SMA_{window}'] + 1e-10)
        df[f'SMA_slope_{window}'] = (df[f'SMA_{window}'] - df[f'SMA_{window}'].shift(4)) / (df[f'SMA_{window}'].shift(4) + 1e-10)
    
    # EMA
    for window in [4, 8, 12, 24, 48, 96]:
        df[f'EMA_{window}'] = df['Close'].shift(1).ewm(span=window, adjust=False).mean()
        df[f'close_EMA_ratio_{window}'] = df['Close'] / (df[f'EMA_{window}'] + 1e-10)
    
    # å‡çº¿äº¤å‰ä¿¡å·
    df['SMA_cross_8_24'] = df['SMA_8'] / (df['SMA_24'] + 1e-10)
    df['SMA_cross_24_96'] = df['SMA_24'] / (df['SMA_96'] + 1e-10)
    df['EMA_cross_12_48'] = df['EMA_12'] / (df['EMA_48'] + 1e-10)
    
    # =============================================
    # 6. æ³¢åŠ¨ç‡ç‰¹å¾ (Volatility Features)
    # =============================================
    for window in [4, 12, 24, 48, 96]:
        # å†å²æ³¢åŠ¨ç‡
        df[f'volatility_{window}'] = df['log_return_1'].shift(1).rolling(window=window).std() * np.sqrt(window)
        
        # ä»·æ ¼èŒƒå›´æ³¢åŠ¨
        df[f'range_volatility_{window}'] = df['price_range'].shift(1).rolling(window=window).mean()
        
        # é«˜ä½ä»·æ³¢åŠ¨
        df[f'hl_volatility_{window}'] = (df['High'] / df['Low']).shift(1).rolling(window=window).std()
    
    # æ³¢åŠ¨ç‡æ¯”ç‡
    df['vol_ratio_4_24'] = df['volatility_4'] / (df['volatility_24'] + 1e-10)
    df['vol_ratio_12_48'] = df['volatility_12'] / (df['volatility_48'] + 1e-10)
    df['vol_ratio_24_96'] = df['volatility_24'] / (df['volatility_96'] + 1e-10)
    
    # Garman-Klassæ³¢åŠ¨ç‡ä¼°è®¡
    log_hl = np.log(df['High'] / df['Low']) ** 2
    log_co = np.log(df['Close'] / df['Open']) ** 2
    df['GK_volatility'] = (0.5 * log_hl - (2 * np.log(2) - 1) * log_co).rolling(window=20).mean()
    
    # Parkinsonæ³¢åŠ¨ç‡
    df['Parkinson_vol'] = np.sqrt(1 / (4 * np.log(2)) * (np.log(df['High'] / df['Low']) ** 2)).rolling(window=20).mean()
    
    # =============================================
    # 7. åŠ¨é‡ç‰¹å¾ (Momentum Features)
    # =============================================
    for period in [4, 8, 12, 24, 48, 96]:
        # åŠ¨é‡
        df[f'momentum_{period}'] = df['Close'] - df['Close'].shift(period)
        df[f'momentum_ratio_{period}'] = df['Close'] / (df['Close'].shift(period) + 1e-10)
        
        # ROC
        df[f'ROC_{period}'] = (df['Close'] - df['Close'].shift(period)) / (df['Close'].shift(period) + 1e-10)
    
    # åŠ¨é‡å˜åŒ–
    df['momentum_12_change'] = df['momentum_12'].diff(4)
    df['momentum_24_change'] = df['momentum_24'].diff(4)
    
    # =============================================
    # 8. æ»åç‰¹å¾ (Lag Features)
    # =============================================
    target_col = 'Target' if is_train and 'Target' in df.columns else 'Close'
    
    # ç›®æ ‡å˜é‡æ»å
    lags = [1, 2, 3, 4, 5, 6, 8, 10, 12, 16, 20, 24, 32, 48, 64, 72, 96]
    for lag in lags:
        df[f'target_lag_{lag}'] = df[target_col].shift(lag)
        if lag <= 48:
            df[f'close_lag_{lag}'] = df['Close'].shift(lag)
            df[f'return_lag_{lag}'] = df['return_1'].shift(lag)
    
    # ç›®æ ‡å·®åˆ†
    for lag in [1, 4, 12, 24, 48, 96]:
        df[f'target_diff_{lag}'] = df[target_col].diff(lag)
    
    # =============================================
    # 9. æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾ (Rolling Statistics)
    # =============================================
    windows = [4, 8, 12, 24, 48, 96]
    
    for window in windows:
        # ç›®æ ‡å˜é‡ç»Ÿè®¡
        df[f'target_rolling_mean_{window}'] = df[target_col].shift(1).rolling(window=window).mean()
        df[f'target_rolling_std_{window}'] = df[target_col].shift(1).rolling(window=window).std()
        df[f'target_rolling_min_{window}'] = df[target_col].shift(1).rolling(window=window).min()
        df[f'target_rolling_max_{window}'] = df[target_col].shift(1).rolling(window=window).max()
        df[f'target_rolling_median_{window}'] = df[target_col].shift(1).rolling(window=window).median()
        df[f'target_rolling_range_{window}'] = df[f'target_rolling_max_{window}'] - df[f'target_rolling_min_{window}']
        
        # æ”¶ç›Šç‡ç»Ÿè®¡
        df[f'return_rolling_mean_{window}'] = df['return_1'].shift(1).rolling(window=window).mean()
        df[f'return_rolling_std_{window}'] = df['return_1'].shift(1).rolling(window=window).std()
        
        # ä»·æ ¼ç»Ÿè®¡
        df[f'close_rolling_mean_{window}'] = df['Close'].shift(1).rolling(window=window).mean()
        df[f'close_rolling_std_{window}'] = df['Close'].shift(1).rolling(window=window).std()
        
        # Z-score
        df[f'target_zscore_{window}'] = (df[target_col] - df[f'target_rolling_mean_{window}']) / (df[f'target_rolling_std_{window}'] + 1e-10)
        df[f'close_zscore_{window}'] = (df['Close'] - df[f'close_rolling_mean_{window}']) / (df[f'close_rolling_std_{window}'] + 1e-10)
    
    # ååº¦å’Œå³°åº¦
    for window in [24, 48, 96]:
        df[f'target_skew_{window}'] = df[target_col].shift(1).rolling(window=window).skew()
        df[f'target_kurt_{window}'] = df[target_col].shift(1).rolling(window=window).kurt()
        df[f'return_skew_{window}'] = df['return_1'].shift(1).rolling(window=window).skew()
        df[f'return_kurt_{window}'] = df['return_1'].shift(1).rolling(window=window).kurt()
    
    # åˆ†ä½æ•°
    for window in [24, 48, 96]:
        df[f'target_q25_{window}'] = df[target_col].shift(1).rolling(window=window).quantile(0.25)
        df[f'target_q75_{window}'] = df[target_col].shift(1).rolling(window=window).quantile(0.75)
        df[f'close_position_{window}'] = (df['Close'] - df[f'close_rolling_mean_{window}'].shift(1).rolling(window).min()) / \
                                         (df[f'close_rolling_mean_{window}'].shift(1).rolling(window).max() - 
                                          df[f'close_rolling_mean_{window}'].shift(1).rolling(window).min() + 1e-10)
    
    # =============================================
    # 10. äº¤å‰ç‰¹å¾ (Interaction Features)
    # =============================================
    # RSIä¸ä»·æ ¼
    df['RSI_return_interact'] = df['RSI_14'] * df['return_1']
    df['RSI_vol_interact'] = df['RSI_14'] * df['volatility_24']
    
    # MACDä¸æˆäº¤é‡
    df['MACD_volume_interact'] = df['MACD_hist'] * df['volume_ratio_24']
    
    # åŠ¨é‡ä¸æ³¢åŠ¨ç‡
    df['momentum_vol_interact'] = df['momentum_12'] * df['volatility_12']
    
    # å¸ƒæ—å¸¦ä¸æˆäº¤é‡
    df['BB_volume_interact'] = df['BB_position_20'] * df['volume_ratio_24']
    
    # ATRä¸ä»·æ ¼
    df['ATR_return_interact'] = df['ATR_ratio_14'] * df['return_1']
    
    # =============================================
    # 11. é«˜çº§ç‰¹å¾ (Advanced Features)
    # =============================================
    # ä¿¡æ¯æ¯”ç‡
    for window in [24, 48, 96]:
        mean_ret = df['return_1'].shift(1).rolling(window=window).mean()
        std_ret = df['return_1'].shift(1).rolling(window=window).std()
        df[f'info_ratio_{window}'] = mean_ret / (std_ret + 1e-10)
    
    # å¤æ™®æ¯”ç‡è¿‘ä¼¼
    for window in [48, 96]:
        excess_ret = df['return_1'].shift(1).rolling(window=window).mean()
        vol = df['return_1'].shift(1).rolling(window=window).std()
        df[f'sharpe_approx_{window}'] = excess_ret / (vol + 1e-10) * np.sqrt(96)
    
    # æœ€å¤§å›æ’¤
    for window in [48, 96]:
        rolling_max = df['Close'].shift(1).rolling(window=window).max()
        df[f'drawdown_{window}'] = (df['Close'] - rolling_max) / (rolling_max + 1e-10)
    
    # è¿ç»­ä¸Šæ¶¨/ä¸‹è·Œå¤©æ•°
    df['up_streak'] = (df['return_1'] > 0).astype(int)
    df['up_streak'] = df['up_streak'].groupby((df['up_streak'] != df['up_streak'].shift()).cumsum()).cumsum()
    
    df['down_streak'] = (df['return_1'] < 0).astype(int)
    df['down_streak'] = df['down_streak'].groupby((df['down_streak'] != df['down_streak'].shift()).cumsum()).cumsum()
    
    return df


# åˆ›å»ºç‰¹å¾
print("åˆ›å»ºè®­ç»ƒé›†ç‰¹å¾...")
train_featured = create_ultimate_features(train_df.copy(), is_train=True)
print(f"ç‰¹å¾åˆ›å»ºå®Œæˆ! åŸå§‹åˆ—æ•°: {train_df.shape[1]}, ç‰¹å¾ååˆ—æ•°: {train_featured.shape[1]}")


# ============================================
# ç¬¬ä¸‰éƒ¨åˆ†: ç‰¹å¾é€‰æ‹©ä¸æ•°æ®å‡†å¤‡
# ============================================
print("\nğŸ¯ ç‰¹å¾é€‰æ‹©ä¸æ•°æ®å‡†å¤‡...")

# æ’é™¤åˆ—
exclude_cols = ['Timestamp', 'Target', 'Open', 'High', 'Low', 'Close', 'Volume',
                'hour', 'day_of_week', 'day_of_month', 'month', 'quarter', 'year',
                'week_of_year', 'day_of_year',
                'OBV', 'OBV_ma_12',
                'BB_upper_10', 'BB_lower_10', 'BB_upper_20', 'BB_lower_20', 
                'BB_upper_40', 'BB_lower_40']

# æ·»åŠ SMA, EMAåˆ°æ’é™¤åˆ— (ä¿ç•™æ¯”ç‡)
for window in [4, 8, 12, 24, 48, 96, 192]:
    exclude_cols.append(f'SMA_{window}')
for window in [4, 8, 12, 24, 48, 96]:
    exclude_cols.append(f'EMA_{window}')

feature_cols = [col for col in train_featured.columns if col not in exclude_cols]
print(f"é€‰æ‹©ç‰¹å¾æ•°é‡: {len(feature_cols)}")

# å¤„ç†æ— ç©·å¤§å’ŒNaN
train_featured = train_featured.replace([np.inf, -np.inf], np.nan)

# åˆ é™¤NaNè¡Œ
valid_idx = train_featured[feature_cols + ['Target']].notna().all(axis=1)
train_clean = train_featured[valid_idx].reset_index(drop=True)
timestamps_clean = train_featured.loc[valid_idx, 'Timestamp'].reset_index(drop=True)

print(f"æ¸…æ´—å‰: {len(train_featured):,} æ ·æœ¬")
print(f"æ¸…æ´—å: {len(train_clean):,} æ ·æœ¬")
print(f"ä¿ç•™æ¯”ä¾‹: {len(train_clean) / len(train_featured) * 100:.2f}%")

# å‡†å¤‡æ•°æ®
X = train_clean[feature_cols].values.astype(np.float32)
y = train_clean['Target'].values.astype(np.float32)

# æ—¶é—´åºåˆ—åˆ†å‰² (80% è®­ç»ƒ, 20% éªŒè¯)
val_ratio = 0.2
train_size = int(len(X) * (1 - val_ratio))

X_train = X[:train_size]
y_train = y[:train_size]
X_val = X[train_size:]
y_val = y[train_size:]

print(f"\nè®­ç»ƒé›†: {X_train.shape[0]:,} æ ·æœ¬")
print(f"éªŒè¯é›†: {X_val.shape[0]:,} æ ·æœ¬")
print(f"è®­ç»ƒé›†æ—¶é—´: {timestamps_clean.iloc[0]} åˆ° {timestamps_clean.iloc[train_size-1]}")
print(f"éªŒè¯é›†æ—¶é—´: {timestamps_clean.iloc[train_size]} åˆ° {timestamps_clean.iloc[-1]}")


# ============================================
# ç¬¬å››éƒ¨åˆ†: å¤šæ¨¡å‹è®­ç»ƒ
# ============================================
print("\nğŸš€ å¼€å§‹å¤šæ¨¡å‹è®­ç»ƒ...")

results = []
trained_models = {}
val_predictions = {}


# --------------- LightGBM ---------------
print("\n" + "=" * 60)
print("è®­ç»ƒæ¨¡å‹ 1: LightGBM (ä¼˜åŒ–å‚æ•°)")
print("=" * 60)

lgb_params = {
    'objective': 'regression',
    'metric': 'rmse',
    'boosting_type': 'gbdt',
    'num_leaves': 63,
    'max_depth': 8,
    'min_child_samples': 100,
    'learning_rate': 0.02,
    'feature_fraction': 0.7,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'reg_alpha': 0.3,
    'reg_lambda': 0.5,
    'verbose': -1,
    'random_state': 42,
    'n_jobs': -1,
    'force_col_wise': True
}

train_data = lgb.Dataset(X_train, label=y_train, feature_name=feature_cols)
val_data = lgb.Dataset(X_val, label=y_val, reference=train_data, feature_name=feature_cols)

evals_result = {}
lgb_model = lgb.train(
    lgb_params,
    train_data,
    num_boost_round=3000,
    valid_sets=[train_data, val_data],
    valid_names=['train', 'valid'],
    callbacks=[
        lgb.early_stopping(stopping_rounds=100),
        lgb.log_evaluation(period=500),
        lgb.record_evaluation(evals_result)
    ]
)

y_pred_lgb = lgb_model.predict(X_val, num_iteration=lgb_model.best_iteration)
rmse_lgb = np.sqrt(mean_squared_error(y_val, y_pred_lgb))
corr_lgb, _ = stats.pearsonr(y_val, y_pred_lgb)

results.append({'Model': 'LightGBM', 'RMSE': rmse_lgb, 'Correlation': corr_lgb, 'Iterations': lgb_model.best_iteration})
trained_models['LightGBM'] = lgb_model
val_predictions['LightGBM'] = y_pred_lgb

print(f"\nLightGBM ç»“æœ:")
print(f"  æœ€ä½³è¿­ä»£: {lgb_model.best_iteration}")
print(f"  RMSE: {rmse_lgb:.6f}")
print(f"  Pearsonç›¸å…³ç³»æ•°: {corr_lgb:.6f}")


# --------------- XGBoost ---------------
print("\n" + "=" * 60)
print("è®­ç»ƒæ¨¡å‹ 2: XGBoost")
print("=" * 60)

xgb_params = {
    'objective': 'reg:squarederror',
    'eval_metric': 'rmse',
    'max_depth': 8,
    'learning_rate': 0.02,
    'n_estimators': 2000,
    'min_child_weight': 100,
    'subsample': 0.8,
    'colsample_bytree': 0.7,
    'reg_alpha': 0.3,
    'reg_lambda': 0.5,
    'random_state': 42,
    'n_jobs': -1,
    'verbosity': 0
}

xgb_model = XGBRegressor(**xgb_params)
xgb_model.set_params(early_stopping_rounds=100)
xgb_model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    verbose=500
)

y_pred_xgb = xgb_model.predict(X_val)
rmse_xgb = np.sqrt(mean_squared_error(y_val, y_pred_xgb))
corr_xgb, _ = stats.pearsonr(y_val, y_pred_xgb)

try:
    xgb_best_iter = xgb_model.best_iteration
except:
    xgb_best_iter = xgb_params['n_estimators']
results.append({'Model': 'XGBoost', 'RMSE': rmse_xgb, 'Correlation': corr_xgb, 'Iterations': xgb_best_iter})
trained_models['XGBoost'] = xgb_model
val_predictions['XGBoost'] = y_pred_xgb

print(f"\nXGBoost ç»“æœ:")
print(f"  æœ€ä½³è¿­ä»£: {xgb_best_iter}")
print(f"  RMSE: {rmse_xgb:.6f}")
print(f"  Pearsonç›¸å…³ç³»æ•°: {corr_xgb:.6f}")


# --------------- LightGBM (DART) ---------------
print("\n" + "=" * 60)
print("è®­ç»ƒæ¨¡å‹ 3: LightGBM (DART)")
print("=" * 60)

lgb_dart_params = {
    'objective': 'regression',
    'metric': 'rmse',
    'boosting_type': 'dart',
    'num_leaves': 47,
    'max_depth': 7,
    'min_child_samples': 150,
    'learning_rate': 0.03,
    'feature_fraction': 0.65,
    'bagging_fraction': 0.75,
    'bagging_freq': 5,
    'reg_alpha': 0.4,
    'reg_lambda': 0.6,
    'drop_rate': 0.1,
    'skip_drop': 0.5,
    'verbose': -1,
    'random_state': 43,
    'n_jobs': -1,
    'force_col_wise': True
}

train_data_dart = lgb.Dataset(X_train, label=y_train, feature_name=feature_cols)
val_data_dart = lgb.Dataset(X_val, label=y_val, reference=train_data_dart, feature_name=feature_cols)

lgb_dart_model = lgb.train(
    lgb_dart_params,
    train_data_dart,
    num_boost_round=1500,
    valid_sets=[train_data_dart, val_data_dart],
    valid_names=['train', 'valid'],
    callbacks=[
        lgb.early_stopping(stopping_rounds=100),
        lgb.log_evaluation(period=500)
    ]
)

y_pred_dart = lgb_dart_model.predict(X_val, num_iteration=lgb_dart_model.best_iteration)
rmse_dart = np.sqrt(mean_squared_error(y_val, y_pred_dart))
corr_dart, _ = stats.pearsonr(y_val, y_pred_dart)

results.append({'Model': 'LightGBM_DART', 'RMSE': rmse_dart, 'Correlation': corr_dart, 'Iterations': lgb_dart_model.best_iteration})
trained_models['LightGBM_DART'] = lgb_dart_model
val_predictions['LightGBM_DART'] = y_pred_dart

print(f"\nLightGBM DART ç»“æœ:")
print(f"  æœ€ä½³è¿­ä»£: {lgb_dart_model.best_iteration}")
print(f"  RMSE: {rmse_dart:.6f}")
print(f"  Pearsonç›¸å…³ç³»æ•°: {corr_dart:.6f}")


# --------------- CatBoost (å°è¯•å¯¼å…¥) ---------------
try:
    from catboost import CatBoostRegressor
    
    print("\n" + "=" * 60)
    print("è®­ç»ƒæ¨¡å‹ 4: CatBoost")
    print("=" * 60)
    
    cat_params = {
        'iterations': 1500,
        'learning_rate': 0.03,
        'depth': 8,
        'l2_leaf_reg': 5,
        'random_seed': 42,
        'verbose': 500,
        'early_stopping_rounds': 100
    }
    
    cat_model = CatBoostRegressor(**cat_params)
    cat_model.fit(X_train, y_train, eval_set=(X_val, y_val))
    
    y_pred_cat = cat_model.predict(X_val)
    rmse_cat = np.sqrt(mean_squared_error(y_val, y_pred_cat))
    corr_cat, _ = stats.pearsonr(y_val, y_pred_cat)
    
    results.append({'Model': 'CatBoost', 'RMSE': rmse_cat, 'Correlation': corr_cat, 'Iterations': cat_model.best_iteration_})
    trained_models['CatBoost'] = cat_model
    val_predictions['CatBoost'] = y_pred_cat
    
    print(f"\nCatBoost ç»“æœ:")
    print(f"  RMSE: {rmse_cat:.6f}")
    print(f"  Pearsonç›¸å…³ç³»æ•°: {corr_cat:.6f}")
    
except ImportError:
    print("\nCatBoost æœªå®‰è£…ï¼Œè·³è¿‡...")


# ============================================
# ç¬¬äº”éƒ¨åˆ†: æ¨¡å‹é›†æˆ
# ============================================
print("\n" + "=" * 60)
print("ğŸ”® æ¨¡å‹é›†æˆ")
print("=" * 60)

# ç»“æœæ±‡æ€»
results_df = pd.DataFrame(results).sort_values('Correlation', ascending=False)
print("\næ¨¡å‹æ€§èƒ½æ±‡æ€» (æŒ‰ç›¸å…³ç³»æ•°æ’åº):")
print(results_df.to_string(index=False))

# è®¡ç®—é›†æˆæƒé‡ (åŸºäºç›¸å…³ç³»æ•°)
weights = {}
total_corr = sum([r['Correlation'] for r in results])
for result in results:
    weights[result['Model']] = result['Correlation'] / total_corr

print("\né›†æˆæƒé‡ (åŸºäºç›¸å…³ç³»æ•°):")
for name, w in sorted(weights.items(), key=lambda x: x[1], reverse=True):
    print(f"  {name}: {w:.4f}")

# åŠ æƒé›†æˆé¢„æµ‹
ensemble_pred = np.zeros(len(y_val))
for name, pred in val_predictions.items():
    ensemble_pred += weights[name] * pred

# è¯„ä¼°é›†æˆæ¨¡å‹
ensemble_rmse = np.sqrt(mean_squared_error(y_val, ensemble_pred))
ensemble_corr, _ = stats.pearsonr(y_val, ensemble_pred)

print(f"\né›†æˆæ¨¡å‹ç»“æœ:")
print(f"  RMSE: {ensemble_rmse:.6f}")
print(f"  Pearsonç›¸å…³ç³»æ•°: {ensemble_corr:.6f}")

# ä¼˜åŒ–æƒé‡ (ç½‘æ ¼æœç´¢)
print("\næœç´¢æœ€ä¼˜é›†æˆæƒé‡...")
best_corr = ensemble_corr
best_weights = weights.copy()

model_names = list(val_predictions.keys())
n_models = len(model_names)

# ç®€å•ç½‘æ ¼æœç´¢
if n_models <= 4:
    for i in range(11):
        for j in range(11 - i):
            for k in range(11 - i - j):
                l = 10 - i - j - k
                if n_models == 3:
                    test_weights = {
                        model_names[0]: i / 10,
                        model_names[1]: j / 10,
                        model_names[2]: (k + l) / 10
                    }
                elif n_models == 4:
                    test_weights = {
                        model_names[0]: i / 10,
                        model_names[1]: j / 10,
                        model_names[2]: k / 10,
                        model_names[3]: l / 10
                    }
                else:
                    continue
                
                test_pred = np.zeros(len(y_val))
                for name, pred in val_predictions.items():
                    test_pred += test_weights[name] * pred
                
                test_corr, _ = stats.pearsonr(y_val, test_pred)
                
                if test_corr > best_corr:
                    best_corr = test_corr
                    best_weights = test_weights.copy()

print(f"\næœ€ä¼˜é›†æˆæƒé‡:")
for name, w in sorted(best_weights.items(), key=lambda x: x[1], reverse=True):
    print(f"  {name}: {w:.4f}")

# ä½¿ç”¨æœ€ä¼˜æƒé‡
final_ensemble_pred = np.zeros(len(y_val))
for name, pred in val_predictions.items():
    final_ensemble_pred += best_weights[name] * pred

final_rmse = np.sqrt(mean_squared_error(y_val, final_ensemble_pred))
final_corr, _ = stats.pearsonr(y_val, final_ensemble_pred)

print(f"\næœ€ç»ˆé›†æˆæ¨¡å‹ç»“æœ:")
print(f"  RMSE: {final_rmse:.6f}")
print(f"  Pearsonç›¸å…³ç³»æ•°: {final_corr:.6f}")


# ============================================
# ç¬¬å…­éƒ¨åˆ†: æµ‹è¯•é›†é¢„æµ‹
# ============================================
print("\n" + "=" * 60)
print("ğŸ“ˆ æµ‹è¯•é›†é¢„æµ‹")
print("=" * 60)

# è·å–å†å²æ•°æ®
max_lag = 400  # éœ€è¦çš„æœ€å¤§å†å²çª—å£
train_tail = train_df.tail(max_lag).copy()

# åˆå¹¶
test_with_history = pd.concat([train_tail, test_df], ignore_index=True)
print(f"è®­ç»ƒé›†å°¾éƒ¨: {len(train_tail)} è¡Œ")
print(f"æµ‹è¯•é›†: {len(test_df)} è¡Œ")

# åˆ›å»ºæµ‹è¯•é›†ç‰¹å¾
print("åˆ›å»ºæµ‹è¯•é›†ç‰¹å¾...")
test_featured = create_ultimate_features(test_with_history.copy(), is_train=False)

# åªä¿ç•™æµ‹è¯•é›†è¡Œ
test_featured = test_featured.tail(len(test_df)).reset_index(drop=True)
print(f"æµ‹è¯•é›†ç‰¹å¾å½¢çŠ¶: {test_featured.shape}")

# ç¡®ä¿æ‰€æœ‰ç‰¹å¾å¯ç”¨
available_features = [col for col in feature_cols if col in test_featured.columns]
missing_features = set(feature_cols) - set(available_features)

if missing_features:
    print(f"ç¼ºå¤±ç‰¹å¾æ•°: {len(missing_features)}")
    for feat in missing_features:
        test_featured[feat] = 0

# å‡†å¤‡æµ‹è¯•ç‰¹å¾
X_test = test_featured[feature_cols].values.astype(np.float32)

# å¤„ç†NaNå’ŒInf
X_test = np.where(np.isinf(X_test), np.nan, X_test)
if np.isnan(X_test).any():
    print("å¤„ç†æµ‹è¯•é›†NaNå€¼...")
    train_means = np.nanmean(X_train, axis=0)
    for i in range(X_test.shape[1]):
        mask = np.isnan(X_test[:, i])
        if mask.any():
            X_test[mask, i] = train_means[i] if not np.isnan(train_means[i]) else 0

print(f"æµ‹è¯•é›†ç‰¹å¾çŸ©é˜µ: {X_test.shape}")
print(f"NaN: {np.isnan(X_test).any()}, Inf: {np.isinf(X_test).any()}")

# é›†æˆé¢„æµ‹
print("\nè¿›è¡Œé›†æˆé¢„æµ‹...")
test_predictions = np.zeros(len(X_test))

for name, model in trained_models.items():
    if name == 'LightGBM':
        pred = model.predict(X_test, num_iteration=model.best_iteration)
    elif name == 'LightGBM_DART':
        pred = model.predict(X_test, num_iteration=model.best_iteration)
    else:
        pred = model.predict(X_test)
    
    test_predictions += best_weights[name] * pred
    print(f"  {name} (æƒé‡ {best_weights[name]:.4f}): å®Œæˆ")

print(f"\né¢„æµ‹å®Œæˆ!")
print(f"é¢„æµ‹æ•°é‡: {len(test_predictions)}")
print(f"é¢„æµ‹å€¼èŒƒå›´: [{test_predictions.min():.6f}, {test_predictions.max():.6f}]")
print(f"é¢„æµ‹å€¼å‡å€¼: {test_predictions.mean():.6f}")
print(f"é¢„æµ‹å€¼æ ‡å‡†å·®: {test_predictions.std():.6f}")


# ============================================
# ç¬¬ä¸ƒéƒ¨åˆ†: åå¤„ç†ä¼˜åŒ–
# ============================================
print("\n" + "=" * 60)
print("ğŸ”§ åå¤„ç†ä¼˜åŒ–")
print("=" * 60)

# ä¿å­˜åŸå§‹é¢„æµ‹(æ— è£å‰ª)
original_predictions = test_predictions.copy()

# 1. æ£€æŸ¥ä¸è®­ç»ƒé›†Targetçš„åˆ†å¸ƒä¸€è‡´æ€§
train_target_mean = y.mean()
train_target_std = y.std()
test_pred_mean = test_predictions.mean()
test_pred_std = test_predictions.std()

print(f"è®­ç»ƒé›†Target: å‡å€¼={train_target_mean:.6f}, æ ‡å‡†å·®={train_target_std:.6f}")
print(f"æµ‹è¯•é›†é¢„æµ‹(åŸå§‹): å‡å€¼={test_pred_mean:.6f}, æ ‡å‡†å·®={test_pred_std:.6f}")

# 2. æ ‡å‡†åŒ–åˆ°è®­ç»ƒåˆ†å¸ƒ (æ¨èæ–¹æ³•)
test_predictions_normalized = (test_predictions - test_pred_mean) / test_pred_std * train_target_std + train_target_mean
print(f"æµ‹è¯•é›†é¢„æµ‹(æ ‡å‡†åŒ–å): å‡å€¼={test_predictions_normalized.mean():.6f}, æ ‡å‡†å·®={test_predictions_normalized.std():.6f}")

# 3. è½»å¾®è£å‰ªæç«¯å€¼ (ä½¿ç”¨åˆ†ä½æ•°)
lower_bound = np.percentile(y, 0.5)
upper_bound = np.percentile(y, 99.5)
test_predictions_clipped = np.clip(test_predictions_normalized, lower_bound, upper_bound)
print(f"è£å‰ªèŒƒå›´(0.5%-99.5%åˆ†ä½): [{lower_bound:.6f}, {upper_bound:.6f}]")

# ä½¿ç”¨æ ‡å‡†åŒ–åçš„é¢„æµ‹ä½œä¸ºæœ€ç»ˆç»“æœ
test_predictions = test_predictions_normalized


# ============================================
# ç¬¬å…«éƒ¨åˆ†: ç”Ÿæˆæäº¤æ–‡ä»¶
# ============================================
print("\n" + "=" * 60)
print("ğŸ“„ ç”Ÿæˆæäº¤æ–‡ä»¶")
print("=" * 60)

# åˆ›å»ºå¤šä¸ªç‰ˆæœ¬çš„æäº¤
submission_dir = Path('submissions')
submission_dir.mkdir(exist_ok=True)

# ç‰ˆæœ¬1: æ ‡å‡†åŒ–ç‰ˆæœ¬
submission_df = pd.DataFrame({
    'Timestamp': test_df['Timestamp'],
    'Prediction': test_predictions_normalized
})
submission_df.to_csv(submission_dir / 'ultimate_ensemble_submission.csv', index=False)
print(f"âœ… æ ‡å‡†åŒ–ç‰ˆæœ¬: submissions/ultimate_ensemble_submission.csv")

# ç‰ˆæœ¬2: è£å‰ªç‰ˆæœ¬
submission_clipped = pd.DataFrame({
    'Timestamp': test_df['Timestamp'],
    'Prediction': test_predictions_clipped
})
submission_clipped.to_csv(submission_dir / 'ultimate_ensemble_clipped.csv', index=False)
print(f"âœ… è£å‰ªç‰ˆæœ¬: submissions/ultimate_ensemble_clipped.csv")

# ç‰ˆæœ¬3: åŸå§‹ç‰ˆæœ¬
submission_original = pd.DataFrame({
    'Timestamp': test_df['Timestamp'],
    'Prediction': original_predictions
})
submission_original.to_csv(submission_dir / 'ultimate_ensemble_original.csv', index=False)
print(f"âœ… åŸå§‹ç‰ˆæœ¬: submissions/ultimate_ensemble_original.csv")

print(f"\næ ‡å‡†åŒ–ç‰ˆæœ¬é¢„è§ˆ:")
print(submission_df.head(10))
print("\n...")
print(submission_df.tail(10))

print(f"\næäº¤æ–‡ä»¶ç»Ÿè®¡:")
print(submission_df['Prediction'].describe())


# ============================================
# ç¬¬ä¹éƒ¨åˆ†: ä¿å­˜æ¨¡å‹å’Œç‰¹å¾
# ============================================
print("\n" + "=" * 60)
print("ğŸ’¾ ä¿å­˜æ¨¡å‹å’Œç‰¹å¾")
print("=" * 60)

model_dir = Path('models')
model_dir.mkdir(exist_ok=True)

# ä¿å­˜LightGBMæ¨¡å‹
lgb_model.save_model(str(model_dir / 'ultimate_lgbm_model.txt'))
print("âœ… LightGBM æ¨¡å‹å·²ä¿å­˜")

# ä¿å­˜ç‰¹å¾åˆ—è¡¨
with open(model_dir / 'ultimate_features.txt', 'w') as f:
    for feat in feature_cols:
        f.write(f"{feat}\n")
print("âœ… ç‰¹å¾åˆ—è¡¨å·²ä¿å­˜")

# ä¿å­˜æ¨¡å‹æƒé‡
with open(model_dir / 'ultimate_weights.txt', 'w') as f:
    for name, w in best_weights.items():
        f.write(f"{name}: {w}\n")
print("âœ… æ¨¡å‹æƒé‡å·²ä¿å­˜")

# ä¿å­˜ç‰¹å¾é‡è¦æ€§
importance = lgb_model.feature_importance(importance_type='gain')
feature_importance_df = pd.DataFrame({
    'feature': feature_cols,
    'importance': importance
}).sort_values('importance', ascending=False)
feature_importance_df.to_csv(model_dir / 'ultimate_feature_importance.csv', index=False)
print("âœ… ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜")


# ============================================
# æ€»ç»“
# ============================================
print("\n" + "=" * 80)
print("ğŸ† ç»ˆæè§£å†³æ–¹æ¡ˆè®­ç»ƒå®Œæˆ!")
print("=" * 80)

print(f"""
æ ¸å¿ƒç­–ç•¥æ€»ç»“:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. ç‰¹å¾å·¥ç¨‹ ({len(feature_cols)} ä¸ªç‰¹å¾):
   â€¢ æ—¶é—´ç‰¹å¾ (å‘¨æœŸæ€§ç¼–ç )
   â€¢ ä»·æ ¼ç‰¹å¾ (OHLCæ¯”ç‡ã€èœ¡çƒ›å›¾å½¢æ€)
   â€¢ æˆäº¤é‡ç‰¹å¾ (OBVã€ä»·é‡å…³ç³»)
   â€¢ æŠ€æœ¯æŒ‡æ ‡ (RSIã€MACDã€å¸ƒæ—å¸¦ã€ATRã€éšæœºæŒ‡æ ‡ç­‰)
   â€¢ ç§»åŠ¨å¹³å‡ (SMAã€EMAã€å‡çº¿äº¤å‰)
   â€¢ æ³¢åŠ¨ç‡ç‰¹å¾ (GKæ³¢åŠ¨ç‡ã€Parkinsonæ³¢åŠ¨ç‡)
   â€¢ åŠ¨é‡ç‰¹å¾ (ROCã€åŠ¨é‡)
   â€¢ æ»åç‰¹å¾ (å¤šæ—¶é—´å°ºåº¦)
   â€¢ æ»šåŠ¨ç»Ÿè®¡ (å‡å€¼ã€æ ‡å‡†å·®ã€ååº¦ã€å³°åº¦ã€åˆ†ä½æ•°)
   â€¢ äº¤å‰ç‰¹å¾ (ç‰¹å¾äº¤äº’)
   â€¢ é«˜çº§ç‰¹å¾ (å¤æ™®æ¯”ç‡ã€æœ€å¤§å›æ’¤)

2. æ¨¡å‹é›†æˆ:
   â€¢ LightGBM (GBDT)
   â€¢ XGBoost
   â€¢ LightGBM (DART)
   â€¢ CatBoost (å¦‚æœå¯ç”¨)

3. é›†æˆç­–ç•¥:
   â€¢ åŸºäºç›¸å…³ç³»æ•°çš„åŠ æƒå¹³å‡
   â€¢ ç½‘æ ¼æœç´¢æœ€ä¼˜æƒé‡

4. éªŒè¯ç»“æœ:
   â€¢ æœ€ç»ˆPearsonç›¸å…³ç³»æ•°: {final_corr:.6f}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æäº¤æ–‡ä»¶: submissions/ultimate_ensemble_submission.csv
""")

print("ç¥ä½ æ¯”èµ›æˆåŠŸ! ğŸš€")
