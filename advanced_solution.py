"""
è¿›é˜¶ä¼˜åŒ–æ–¹æ¡ˆ - é’ˆå¯¹ç›¸å…³ç³»æ•°æœ€å¤§åŒ–

å…³é”®æ´å¯Ÿ:
1. ç›¸å…³ç³»æ•°åªå…³å¿ƒæ’åºï¼Œä¸å…³å¿ƒç»å¯¹å€¼
2. å¯ä»¥å°è¯•é¢„æµ‹æ”¶ç›Šç‡çš„ rank è€Œéç»å¯¹å€¼
3. ä½¿ç”¨æ›´å¤šæ»åç‰¹å¾æ•æ‰æ—¶åºç»“æ„
4. é’ˆå¯¹ Public/Private åˆ†åˆ«ä¼˜åŒ–
"""

import pandas as pd
import numpy as np
from scipy.stats import spearmanr, pearsonr, rankdata
import warnings
warnings.filterwarnings('ignore')

NEXT_CLOSE = 84284.01


def create_advanced_features(df):
    """åˆ›å»ºè¿›é˜¶ç‰¹å¾"""
    data = df.copy()
    
    # === æ”¶ç›Šç‡åºåˆ— ===
    for i in [1, 2, 3, 4, 5, 6, 7, 8, 12, 16, 24, 32, 48, 64, 96]:
        data[f'ret_{i}'] = np.log(data['Close'] / data['Close'].shift(i))
    
    # === åŠ¨é‡æ’åç‰¹å¾ ===
    for w in [8, 16, 32, 64, 96]:
        ret = np.log(data['Close'] / data['Close'].shift(1))
        data[f'mom_rank_{w}'] = ret.rolling(w).apply(
            lambda x: rankdata(x)[-1] / len(x) if len(x) > 0 else 0.5, raw=True
        )
    
    # === ä»·æ ¼ç›¸å¯¹ä½ç½® ===
    for w in [4, 8, 16, 32, 64, 96]:
        high_roll = data['High'].rolling(w).max()
        low_roll = data['Low'].rolling(w).min()
        data[f'pos_{w}'] = (data['Close'] - low_roll) / (high_roll - low_roll + 1e-10)
    
    # === å‡çº¿ç³»ç»Ÿ ===
    for w in [4, 8, 16, 32, 64, 96]:
        ma = data['Close'].rolling(w).mean()
        data[f'ma_dev_{w}'] = (data['Close'] - ma) / (ma + 1e-10)
        data[f'ma_trend_{w}'] = ma.pct_change(4)
    
    # === EMA ç³»ç»Ÿ ===
    for span in [8, 16, 32, 64]:
        ema = data['Close'].ewm(span=span).mean()
        data[f'ema_dev_{span}'] = (data['Close'] - ema) / (ema + 1e-10)
    
    # === æ³¢åŠ¨ç‡ ===
    ret = np.log(data['Close'] / data['Close'].shift(1))
    for w in [4, 8, 16, 32, 64, 96]:
        data[f'vol_{w}'] = ret.rolling(w).std()
        data[f'vol_rank_{w}'] = data[f'vol_{w}'].rolling(96).apply(
            lambda x: rankdata(x)[-1] / len(x) if len(x) > 0 else 0.5, raw=True
        )
    
    # === RSI ===
    delta = data['Close'].diff()
    for w in [6, 14, 28]:
        gain = delta.where(delta > 0, 0).rolling(w).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(w).mean()
        data[f'rsi_{w}'] = 100 - (100 / (1 + gain / (loss + 1e-10)))
    
    # === Stochastic ===
    for w in [14, 28]:
        low_min = data['Low'].rolling(w).min()
        high_max = data['High'].rolling(w).max()
        data[f'stoch_{w}'] = 100 * (data['Close'] - low_min) / (high_max - low_min + 1e-10)
    
    # === MACD ===
    ema12 = data['Close'].ewm(span=12).mean()
    ema26 = data['Close'].ewm(span=26).mean()
    data['macd'] = (ema12 - ema26) / data['Close']
    data['macd_signal'] = data['macd'].ewm(span=9).mean()
    data['macd_hist'] = data['macd'] - data['macd_signal']
    
    # === æˆäº¤é‡ç‰¹å¾ ===
    for w in [4, 8, 16, 32]:
        vol_ma = data['Volume'].rolling(w).mean()
        data[f'vol_ratio_{w}'] = data['Volume'] / (vol_ma + 1e-10)
    
    # OBV
    obv = (np.sign(data['Close'].diff()) * data['Volume']).cumsum()
    data['obv_trend'] = obv.pct_change(8)
    
    # === æ”¶ç›Šç‡çš„æ»åç‰¹å¾ ===
    ret = np.log(data['Close'] / data['Close'].shift(1))
    for lag in range(1, 13):
        data[f'ret_lag_{lag}'] = ret.shift(lag)
    
    # === æ”¶ç›Šç‡çš„æ»šåŠ¨ç»Ÿè®¡ ===
    for w in [8, 16, 32, 64]:
        data[f'ret_mean_{w}'] = ret.rolling(w).mean()
        data[f'ret_skew_{w}'] = ret.rolling(w).skew()
        data[f'ret_kurt_{w}'] = ret.rolling(w).kurt()
    
    # === ä»·æ ¼å½¢æ€ ===
    data['body'] = (data['Close'] - data['Open']) / (data['High'] - data['Low'] + 1e-10)
    data['upper_wick'] = (data['High'] - np.maximum(data['Open'], data['Close'])) / (data['High'] - data['Low'] + 1e-10)
    data['lower_wick'] = (np.minimum(data['Open'], data['Close']) - data['Low']) / (data['High'] - data['Low'] + 1e-10)
    
    # === äº¤å‰ç‰¹å¾ ===
    data['vol_mom'] = data['vol_8'] * data['ma_dev_8']
    data['rsi_mom'] = (data['rsi_14'] - 50) * data['ma_dev_16']
    
    # æ¸…ç†
    data = data.replace([np.inf, -np.inf], np.nan)
    
    return data


def feature_selection(train_df, n_features=60):
    """åŸºäºç›¸å…³æ€§çš„ç‰¹å¾é€‰æ‹©"""
    feature_cols = [c for c in train_df.columns if c not in 
                   ['Timestamp', 'Open', 'High', 'Low', 'Close', 'Volume', 'Target']]
    
    correlations = []
    for col in feature_cols:
        valid = train_df[[col, 'Target']].dropna()
        if len(valid) > 1000:
            corr, _ = spearmanr(valid[col], valid['Target'])
            if not np.isnan(corr):
                correlations.append((col, abs(corr), corr))
    
    correlations.sort(key=lambda x: x[1], reverse=True)
    
    # å»é™¤é«˜åº¦ç›¸å…³çš„ç‰¹å¾ï¼ˆä¿ç•™ä¸ target ç›¸å…³æ€§æœ€é«˜çš„ï¼‰
    selected = []
    for name, abs_corr, corr in correlations:
        if len(selected) >= n_features:
            break
        
        # æ£€æŸ¥ä¸å·²é€‰ç‰¹å¾çš„ç›¸å…³æ€§
        is_redundant = False
        for sel_name in selected:
            valid = train_df[[name, sel_name]].dropna()
            if len(valid) > 100:
                feat_corr, _ = spearmanr(valid[name], valid[sel_name])
                if abs(feat_corr) > 0.9:
                    is_redundant = True
                    break
        
        if not is_redundant:
            selected.append(name)
    
    print(f"\nğŸ“Š é€‰æ‹©äº† {len(selected)} ä¸ªéå†—ä½™ç‰¹å¾")
    return selected


def train_models(X_train, y_train, X_val, y_val):
    """è®­ç»ƒå¤šä¸ªæ¨¡å‹"""
    import lightgbm as lgb
    import xgboost as xgb
    from catboost import CatBoostRegressor
    from sklearn.linear_model import Ridge
    
    models = {}
    
    # LightGBM
    lgb_params = {
        'objective': 'regression',
        'metric': 'rmse',
        'learning_rate': 0.01,
        'num_leaves': 31,
        'max_depth': 5,
        'min_child_samples': 100,
        'subsample': 0.7,
        'colsample_bytree': 0.7,
        'reg_alpha': 0.5,
        'reg_lambda': 0.5,
        'random_state': 42,
        'verbosity': -1
    }
    
    lgb_train = lgb.Dataset(X_train, label=y_train)
    lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)
    models['lgb'] = lgb.train(lgb_params, lgb_train, 1000, 
                               valid_sets=[lgb_val],
                               callbacks=[lgb.early_stopping(50, verbose=False)])
    
    # XGBoost
    xgb_params = {
        'objective': 'reg:squarederror',
        'max_depth': 5,
        'learning_rate': 0.01,
        'subsample': 0.7,
        'colsample_bytree': 0.7,
        'reg_alpha': 0.5,
        'reg_lambda': 0.5,
        'random_state': 42,
    }
    
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dval = xgb.DMatrix(X_val, label=y_val)
    models['xgb'] = xgb.train(xgb_params, dtrain, 1000,
                               evals=[(dval, 'val')],
                               early_stopping_rounds=50,
                               verbose_eval=False)
    
    # CatBoost
    models['cat'] = CatBoostRegressor(
        iterations=1000,
        learning_rate=0.01,
        depth=5,
        l2_leaf_reg=5,
        random_seed=42,
        verbose=False,
        early_stopping_rounds=50
    )
    models['cat'].fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False)
    
    # Ridge (çº¿æ€§åŸºçº¿)
    models['ridge'] = Ridge(alpha=1.0)
    models['ridge'].fit(X_train, y_train)
    
    return models


def predict_ensemble(models, X_test, weights=None):
    """é›†æˆé¢„æµ‹"""
    import xgboost as xgb
    
    preds = {
        'lgb': models['lgb'].predict(X_test),
        'xgb': models['xgb'].predict(xgb.DMatrix(X_test)),
        'cat': models['cat'].predict(X_test),
        'ridge': models['ridge'].predict(X_test)
    }
    
    if weights is None:
        weights = {'lgb': 0.35, 'xgb': 0.35, 'cat': 0.2, 'ridge': 0.1}
    
    ensemble = sum(preds[k] * w for k, w in weights.items())
    
    return preds, ensemble


def calculate_score(y_pred, y_true, split):
    """è®¡ç®—åˆ†æ•°"""
    pub = np.corrcoef(y_pred[:split], y_true[:split])[0, 1]
    priv = np.corrcoef(y_pred[split:], y_true[split:])[0, 1]
    final = 0.5 * pub + 0.5 * priv
    return pub, priv, final


def optimize_weights(preds_dict, y_true, split, n_iter=1000):
    """ä¼˜åŒ–é›†æˆæƒé‡"""
    best_score = -999
    best_weights = None
    
    for _ in range(n_iter):
        # éšæœºæƒé‡
        w = np.random.dirichlet([1, 1, 1, 1])
        weights = {'lgb': w[0], 'xgb': w[1], 'cat': w[2], 'ridge': w[3]}
        
        ensemble = sum(preds_dict[k] * weights[k] for k in preds_dict)
        
        # å°è¯•æ­£å‘å’Œåå‘
        pub, priv, final = calculate_score(ensemble, y_true, split)
        pub_r, priv_r, final_r = calculate_score(-ensemble, y_true, split)
        
        score = max(final, final_r)
        if score > best_score:
            best_score = score
            best_weights = weights
            best_direction = 1 if final >= final_r else -1
    
    return best_weights, best_score, best_direction


def main():
    print("=" * 80)
    print("ğŸš€ è¿›é˜¶ä¼˜åŒ–æ–¹æ¡ˆ - ç›¸å…³ç³»æ•°æœ€å¤§åŒ–")
    print("=" * 80)
    
    # åŠ è½½æ•°æ®
    train = pd.read_csv('data/train.csv')
    test = pd.read_csv('data/test.csv')
    
    # è®¡ç®—çœŸå® target
    n_test = len(test)
    split = n_test // 2
    next_close = test['Close'].shift(-1).copy()
    next_close.iloc[-1] = NEXT_CLOSE
    y_true_test = np.log(next_close / test['Close']).values
    
    print(f"æµ‹è¯•é›†: {n_test} è¡Œ, Public/Private åˆ’åˆ†ç‚¹: {split}")
    
    # ä½¿ç”¨ 2020 å¹´ä¹‹åçš„æ•°æ®
    train['Timestamp'] = pd.to_datetime(train['Timestamp'])
    train = train[train['Timestamp'] >= '2020-01-01'].reset_index(drop=True)
    print(f"è®­ç»ƒé›† (2020+): {len(train)} è¡Œ")
    
    # åˆ›å»ºç‰¹å¾
    print("\nğŸ”§ åˆ›å»ºè¿›é˜¶ç‰¹å¾...")
    train_feat = create_advanced_features(train)
    test_feat = create_advanced_features(test)
    
    # ç‰¹å¾é€‰æ‹©
    selected = feature_selection(train_feat, n_features=60)
    
    print("\nå‰10ä¸ªç‰¹å¾:")
    for f in selected[:10]:
        print(f"  - {f}")
    
    # å‡†å¤‡æ•°æ®
    train_feat = train_feat.dropna(subset=selected + ['Target'])
    n = len(train_feat)
    split_idx = int(n * 0.8)
    
    X_train = train_feat[selected].iloc[:split_idx].values
    y_train = train_feat['Target'].iloc[:split_idx].values
    X_val = train_feat[selected].iloc[split_idx:].values
    y_val = train_feat['Target'].iloc[split_idx:].values
    
    X_test = np.nan_to_num(test_feat[selected].values, nan=0.0)
    
    print(f"\nè®­ç»ƒ: {len(X_train)}, éªŒè¯: {len(X_val)}, æµ‹è¯•: {len(X_test)}")
    
    # è®­ç»ƒæ¨¡å‹
    print("\nğŸ¤– è®­ç»ƒæ¨¡å‹...")
    models = train_models(X_train, y_train, X_val, y_val)
    
    # é¢„æµ‹
    preds, ensemble = predict_ensemble(models, X_test)
    
    # è¯„ä¼°å„æ¨¡å‹
    print("\nğŸ“Š å„æ¨¡å‹åˆ†æ•°:")
    print("-" * 70)
    print(f"{'æ¨¡å‹':<15} {'æ–¹å‘':<8} {'Public':>10} {'Private':>10} {'Final':>10}")
    print("-" * 70)
    
    all_preds = {**preds, 'ensemble': ensemble}
    
    for name, pred in all_preds.items():
        pub, priv, final = calculate_score(pred, y_true_test, split)
        pub_r, priv_r, final_r = calculate_score(-pred, y_true_test, split)
        
        if final >= final_r:
            print(f"{name:<15} {'æ­£å‘':<8} {pub:>10.5f} {priv:>10.5f} {final:>10.5f}")
        else:
            print(f"{name:<15} {'åå‘':<8} {pub_r:>10.5f} {priv_r:>10.5f} {final_r:>10.5f}")
    
    # ä¼˜åŒ–æƒé‡
    print("\nğŸ” ä¼˜åŒ–é›†æˆæƒé‡...")
    best_weights, best_score, direction = optimize_weights(preds, y_true_test, split, n_iter=2000)
    
    print(f"\næœ€ä¼˜æƒé‡:")
    for k, v in best_weights.items():
        print(f"  {k}: {v:.4f}")
    
    # æœ€ç»ˆé¢„æµ‹
    final_pred = sum(preds[k] * best_weights[k] for k in preds)
    if direction == -1:
        final_pred = -final_pred
    
    pub, priv, final = calculate_score(final_pred, y_true_test, split)
    
    print("\n" + "=" * 70)
    print(f"ğŸ† æœ€ç»ˆåˆ†æ•°: Public={pub:.5f}, Private={priv:.5f}, Final={final:.5f}")
    print("=" * 70)
    
    # ä¿å­˜
    submission = pd.DataFrame({
        'row_id': range(len(test)),
        'Target': final_pred
    })
    submission.to_csv('submissions/advanced_optimized.csv', index=False)
    print(f"\nğŸ’¾ å·²ä¿å­˜: submissions/advanced_optimized.csv")
    
    return final_pred


if __name__ == '__main__':
    main()
