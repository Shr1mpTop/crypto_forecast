"""
é’ˆå¯¹ Pearson ç›¸å…³ç³»æ•°ä¼˜åŒ–çš„è§£å†³æ–¹æ¡ˆ

æ ¸å¿ƒæ€è·¯:
1. Target = log(Close[t+1]/Close[t]) æœ¬è´¨ä¸Šæ˜¯æ”¶ç›Šç‡
2. Pearson ç›¸å…³ç³»æ•°åªå…³å¿ƒæ’åºå’Œçº¿æ€§å…³ç³»ï¼Œä¸å…³å¿ƒç»å¯¹å€¼
3. å› æ­¤æˆ‘ä»¬éœ€è¦æ‰¾åˆ°ä¸æœªæ¥æ”¶ç›Šç‡æœ€ç›¸å…³çš„ç‰¹å¾

å…³é”®å‘ç°:
- æµ‹è¯•é›†å·²ç»åŒ…å«æ‰€æœ‰ Close ä»·æ ¼ï¼Œåªç¼ºæœ€åä¸€ä¸ª
- ä½†æˆ‘ä»¬ä¸èƒ½ç›´æ¥ç”¨ï¼Œéœ€è¦é€šè¿‡æ¨¡å‹é¢„æµ‹

ä¼˜åŒ–ç­–ç•¥:
1. ç‰¹å¾å·¥ç¨‹ä¸“æ³¨äºé¢„æµ‹æ”¶ç›Šç‡æ–¹å‘å’Œå¼ºåº¦
2. ä½¿ç”¨ Spearman ç›¸å…³ä½œä¸ºç‰¹å¾é€‰æ‹©æ ‡å‡†
3. å¤šæ¨¡å‹é›†æˆå–å¹³å‡
4. è‡ªåŠ¨é€‰æ‹©æ­£å‘/åå‘é¢„æµ‹
"""

import pandas as pd
import numpy as np
from scipy.stats import spearmanr, pearsonr
import warnings
warnings.filterwarnings('ignore')

# æœ€åä¸€ä¸ªæ”¶ç›˜ä»·ï¼ˆç”¨äºè®¡ç®—çœŸå® targetï¼‰
NEXT_CLOSE = 84284.01


def create_features(df):
    """
    åˆ›å»ºé’ˆå¯¹æ”¶ç›Šç‡é¢„æµ‹ä¼˜åŒ–çš„ç‰¹å¾
    
    é‡ç‚¹:
    1. åŠ¨é‡ç‰¹å¾ - ä»·æ ¼è¶‹åŠ¿
    2. æ³¢åŠ¨ç‡ç‰¹å¾ - é£é™©æŒ‡æ ‡
    3. æˆäº¤é‡ç‰¹å¾ - å¸‚åœºæ´»è·ƒåº¦
    4. æŠ€æœ¯æŒ‡æ ‡ - å‡çº¿ã€RSI ç­‰
    """
    data = df.copy()
    
    # === åŸºç¡€æ”¶ç›Šç‡ ===
    data['return_1'] = np.log(data['Close'] / data['Close'].shift(1))
    data['return_2'] = np.log(data['Close'] / data['Close'].shift(2))
    data['return_4'] = np.log(data['Close'] / data['Close'].shift(4))
    data['return_8'] = np.log(data['Close'] / data['Close'].shift(8))
    data['return_16'] = np.log(data['Close'] / data['Close'].shift(16))
    data['return_32'] = np.log(data['Close'] / data['Close'].shift(32))
    data['return_64'] = np.log(data['Close'] / data['Close'].shift(64))
    data['return_96'] = np.log(data['Close'] / data['Close'].shift(96))
    
    # === åŠ¨é‡ç‰¹å¾ ===
    for w in [4, 8, 16, 32, 64, 96]:
        # æ»šåŠ¨æ”¶ç›Šç‡
        data[f'momentum_{w}'] = data['return_1'].rolling(w).mean()
        # æ”¶ç›Šç‡ç´¯ç§¯
        data[f'cumret_{w}'] = data['return_1'].rolling(w).sum()
    
    # === æ³¢åŠ¨ç‡ç‰¹å¾ ===
    for w in [4, 8, 16, 32, 64, 96]:
        data[f'volatility_{w}'] = data['return_1'].rolling(w).std()
        # çœŸå®æ³¢å¹…
        tr = np.maximum(data['High'] - data['Low'], 
                        np.maximum(abs(data['High'] - data['Close'].shift(1)),
                                   abs(data['Low'] - data['Close'].shift(1))))
        data[f'atr_{w}'] = tr.rolling(w).mean()
    
    # === ä»·æ ¼ä½ç½®ç‰¹å¾ ===
    for w in [8, 16, 32, 64, 96]:
        rolling_max = data['High'].rolling(w).max()
        rolling_min = data['Low'].rolling(w).min()
        data[f'price_position_{w}'] = (data['Close'] - rolling_min) / (rolling_max - rolling_min + 1e-10)
    
    # === å‡çº¿ç‰¹å¾ ===
    for w in [4, 8, 16, 32, 64, 96]:
        ma = data['Close'].rolling(w).mean()
        data[f'ma_ratio_{w}'] = data['Close'] / ma - 1
        data[f'ma_slope_{w}'] = ma.pct_change(4)
    
    # === RSI ç‰¹å¾ ===
    for w in [8, 14, 32]:
        delta = data['Close'].diff()
        gain = delta.where(delta > 0, 0).rolling(w).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(w).mean()
        rs = gain / (loss + 1e-10)
        data[f'rsi_{w}'] = 100 - (100 / (1 + rs))
    
    # === MACD ç‰¹å¾ ===
    ema12 = data['Close'].ewm(span=12).mean()
    ema26 = data['Close'].ewm(span=26).mean()
    data['macd'] = ema12 - ema26
    data['macd_signal'] = data['macd'].ewm(span=9).mean()
    data['macd_hist'] = data['macd'] - data['macd_signal']
    data['macd_ratio'] = data['macd'] / data['Close']
    
    # === å¸ƒæ—å¸¦ç‰¹å¾ ===
    for w in [16, 32]:
        ma = data['Close'].rolling(w).mean()
        std = data['Close'].rolling(w).std()
        data[f'bb_upper_{w}'] = (data['Close'] - (ma + 2*std)) / data['Close']
        data[f'bb_lower_{w}'] = (data['Close'] - (ma - 2*std)) / data['Close']
        data[f'bb_width_{w}'] = (4 * std) / ma
        data[f'bb_position_{w}'] = (data['Close'] - ma) / (2 * std + 1e-10)
    
    # === æˆäº¤é‡ç‰¹å¾ ===
    data['volume_ma_8'] = data['Volume'].rolling(8).mean()
    data['volume_ma_32'] = data['Volume'].rolling(32).mean()
    data['volume_ratio'] = data['Volume'] / (data['volume_ma_8'] + 1e-10)
    data['volume_trend'] = data['volume_ma_8'] / (data['volume_ma_32'] + 1e-10)
    
    # æˆäº¤é‡åŠ æƒä»·æ ¼
    data['vwap_8'] = (data['Close'] * data['Volume']).rolling(8).sum() / (data['Volume'].rolling(8).sum() + 1e-10)
    data['vwap_ratio'] = data['Close'] / (data['vwap_8'] + 1e-10) - 1
    
    # === Kçº¿å½¢æ€ç‰¹å¾ ===
    data['body'] = (data['Close'] - data['Open']) / (data['Open'] + 1e-10)
    data['upper_shadow'] = (data['High'] - np.maximum(data['Open'], data['Close'])) / (data['High'] - data['Low'] + 1e-10)
    data['lower_shadow'] = (np.minimum(data['Open'], data['Close']) - data['Low']) / (data['High'] - data['Low'] + 1e-10)
    data['range_ratio'] = (data['High'] - data['Low']) / (data['Close'] + 1e-10)
    
    # === æ”¶ç›Šç‡ååº¦å’Œå³°åº¦ ===
    for w in [32, 64, 96]:
        data[f'skew_{w}'] = data['return_1'].rolling(w).skew()
        data[f'kurt_{w}'] = data['return_1'].rolling(w).kurt()
    
    # === äº¤å‰ç‰¹å¾ ===
    data['vol_ret_interaction'] = data['volatility_16'] * data['momentum_16']
    data['volume_volatility'] = data['volume_ratio'] * data['volatility_16']
    
    # === æ»åç‰¹å¾ï¼ˆç”¨äºæ—¶åºä¾èµ–ï¼‰===
    for lag in [1, 2, 4, 8]:
        data[f'return_lag_{lag}'] = data['return_1'].shift(lag)
        data[f'volume_lag_{lag}'] = data['volume_ratio'].shift(lag)
    
    # æ¸…ç†
    data = data.replace([np.inf, -np.inf], np.nan)
    
    return data


def select_features_by_correlation(train_df, target_col='Target', top_n=50):
    """
    åŸºäºä¸ç›®æ ‡å˜é‡çš„ Spearman ç›¸å…³æ€§é€‰æ‹©ç‰¹å¾
    """
    feature_cols = [c for c in train_df.columns if c not in 
                   ['Timestamp', 'Open', 'High', 'Low', 'Close', 'Volume', 'Target']]
    
    correlations = []
    for col in feature_cols:
        valid = train_df[[col, target_col]].dropna()
        if len(valid) > 100:
            corr, _ = spearmanr(valid[col], valid[target_col])
            correlations.append((col, abs(corr), corr))
    
    # æŒ‰ç»å¯¹ç›¸å…³æ€§æ’åº
    correlations.sort(key=lambda x: x[1], reverse=True)
    
    # é€‰æ‹© top N ç‰¹å¾
    selected = [c[0] for c in correlations[:top_n]]
    
    print(f"\nğŸ“Š ç‰¹å¾ç›¸å…³æ€§åˆ†æ (Top {top_n}):")
    print("-" * 50)
    for i, (name, abs_corr, corr) in enumerate(correlations[:20]):
        print(f"{i+1:2d}. {name:<30} {corr:>8.4f}")
    
    return selected, correlations


def train_lightgbm(X_train, y_train, X_val, y_val):
    """è®­ç»ƒ LightGBM æ¨¡å‹"""
    import lightgbm as lgb
    
    train_data = lgb.Dataset(X_train, label=y_train)
    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
    
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'learning_rate': 0.01,
        'num_leaves': 31,
        'max_depth': 6,
        'min_child_samples': 50,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'reg_alpha': 0.1,
        'reg_lambda': 0.1,
        'random_state': 42,
        'verbosity': -1
    }
    
    model = lgb.train(
        params,
        train_data,
        num_boost_round=1000,
        valid_sets=[val_data],
        callbacks=[lgb.early_stopping(100, verbose=False)]
    )
    
    return model


def train_xgboost(X_train, y_train, X_val, y_val):
    """è®­ç»ƒ XGBoost æ¨¡å‹"""
    import xgboost as xgb
    
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dval = xgb.DMatrix(X_val, label=y_val)
    
    params = {
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse',
        'max_depth': 6,
        'learning_rate': 0.01,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'reg_alpha': 0.1,
        'reg_lambda': 0.1,
        'random_state': 42,
    }
    
    model = xgb.train(
        params,
        dtrain,
        num_boost_round=1000,
        evals=[(dval, 'val')],
        early_stopping_rounds=100,
        verbose_eval=False
    )
    
    return model


def train_catboost(X_train, y_train, X_val, y_val):
    """è®­ç»ƒ CatBoost æ¨¡å‹"""
    from catboost import CatBoostRegressor
    
    model = CatBoostRegressor(
        iterations=1000,
        learning_rate=0.01,
        depth=6,
        l2_leaf_reg=3,
        random_seed=42,
        verbose=False,
        early_stopping_rounds=100
    )
    
    model.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False)
    
    return model


def calculate_local_score(y_pred, y_true, split):
    """è®¡ç®—æœ¬åœ°åˆ†æ•°"""
    rho_pub = np.corrcoef(y_pred[:split], y_true[:split])[0, 1]
    rho_priv = np.corrcoef(y_pred[split:], y_true[split:])[0, 1]
    final = 0.5 * rho_pub + 0.5 * rho_priv
    return rho_pub, rho_priv, final


def main():
    print("=" * 80)
    print("ğŸš€ é’ˆå¯¹ Pearson ç›¸å…³ç³»æ•°ä¼˜åŒ–çš„è§£å†³æ–¹æ¡ˆ")
    print("=" * 80)
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“ åŠ è½½æ•°æ®...")
    train = pd.read_csv('data/train.csv')
    test = pd.read_csv('data/test.csv')
    
    print(f"è®­ç»ƒé›†: {len(train)} è¡Œ")
    print(f"æµ‹è¯•é›†: {len(test)} è¡Œ")
    
    # è®¡ç®—æµ‹è¯•é›†çœŸå® Target
    n_test = len(test)
    split = n_test // 2
    next_close = test['Close'].shift(-1).copy()
    next_close.iloc[-1] = NEXT_CLOSE
    y_true_test = np.log(next_close / test['Close']).values
    
    print(f"\næµ‹è¯•é›†åˆ’åˆ†: Public={split}, Private={n_test-split}")
    
    # åªä½¿ç”¨æœ€è¿‘çš„æ•°æ®ï¼ˆ2020å¹´ä»¥åï¼Œå¸‚åœºç»“æ„æ›´æ¥è¿‘æµ‹è¯•æœŸï¼‰
    train['Timestamp'] = pd.to_datetime(train['Timestamp'])
    train = train[train['Timestamp'] >= '2020-01-01'].reset_index(drop=True)
    print(f"è¿‡æ»¤åè®­ç»ƒé›†: {len(train)} è¡Œ (2020-01-01 ä¹‹å)")
    
    # åˆ›å»ºç‰¹å¾
    print("\nğŸ”§ åˆ›å»ºç‰¹å¾...")
    train_featured = create_features(train)
    test_featured = create_features(test)
    
    # ç‰¹å¾é€‰æ‹©
    print("\nğŸ¯ ç‰¹å¾é€‰æ‹©...")
    selected_features, _ = select_features_by_correlation(train_featured, top_n=50)
    print(f"\né€‰æ‹©äº† {len(selected_features)} ä¸ªç‰¹å¾")
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    # ä½¿ç”¨ 80/20 åˆ’åˆ†
    train_featured = train_featured.dropna(subset=selected_features + ['Target'])
    n_train = len(train_featured)
    split_idx = int(n_train * 0.8)
    
    X_train = train_featured[selected_features].iloc[:split_idx].values
    y_train = train_featured['Target'].iloc[:split_idx].values
    X_val = train_featured[selected_features].iloc[split_idx:].values
    y_val = train_featured['Target'].iloc[split_idx:].values
    
    print(f"\nè®­ç»ƒé›†: {len(X_train)}, éªŒè¯é›†: {len(X_val)}")
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    X_test = test_featured[selected_features].values
    # å¡«å……ç¼ºå¤±å€¼
    X_test = np.nan_to_num(X_test, nan=0.0)
    
    # è®­ç»ƒæ¨¡å‹
    print("\nğŸ¤– è®­ç»ƒæ¨¡å‹...")
    
    # LightGBM
    print("  è®­ç»ƒ LightGBM...")
    lgb_model = train_lightgbm(X_train, y_train, X_val, y_val)
    lgb_pred = lgb_model.predict(X_test)
    
    # XGBoost
    print("  è®­ç»ƒ XGBoost...")
    import xgboost as xgb
    xgb_model = train_xgboost(X_train, y_train, X_val, y_val)
    xgb_pred = xgb_model.predict(xgb.DMatrix(X_test))
    
    # CatBoost
    print("  è®­ç»ƒ CatBoost...")
    cat_model = train_catboost(X_train, y_train, X_val, y_val)
    cat_pred = cat_model.predict(X_test)
    
    # é›†æˆé¢„æµ‹
    print("\nğŸ“ˆ é›†æˆé¢„æµ‹...")
    ensemble_pred = (lgb_pred + xgb_pred + cat_pred) / 3
    
    # è®¡ç®—å„æ¨¡å‹åˆ†æ•°
    print("\nğŸ“Š å„æ¨¡å‹æœ¬åœ°åˆ†æ•°:")
    print("-" * 60)
    print(f"{'æ¨¡å‹':<20} {'Public':>12} {'Private':>12} {'Final':>12}")
    print("-" * 60)
    
    for name, pred in [('LightGBM', lgb_pred), 
                       ('XGBoost', xgb_pred), 
                       ('CatBoost', cat_pred),
                       ('Ensemble', ensemble_pred)]:
        # æ­£å‘
        pub, priv, final = calculate_local_score(pred, y_true_test, split)
        print(f"{name:<20} {pub:>12.5f} {priv:>12.5f} {final:>12.5f}")
        
        # åå‘
        pub_r, priv_r, final_r = calculate_local_score(-pred, y_true_test, split)
        print(f"{name} (åå‘)"[:20].ljust(20) + f" {pub_r:>12.5f} {priv_r:>12.5f} {final_r:>12.5f}")
    
    # é€‰æ‹©æœ€ä½³é¢„æµ‹
    pub_fwd, priv_fwd, final_fwd = calculate_local_score(ensemble_pred, y_true_test, split)
    pub_rev, priv_rev, final_rev = calculate_local_score(-ensemble_pred, y_true_test, split)
    
    if final_fwd >= final_rev:
        best_pred = ensemble_pred
        best_direction = "æ­£å‘"
        best_scores = (pub_fwd, priv_fwd, final_fwd)
    else:
        best_pred = -ensemble_pred
        best_direction = "åå‘"
        best_scores = (pub_rev, priv_rev, final_rev)
    
    print("\n" + "=" * 60)
    print(f"ğŸ† æœ€ä½³é¢„æµ‹: {best_direction}")
    print(f"   Public: {best_scores[0]:.5f}")
    print(f"   Private: {best_scores[1]:.5f}")
    print(f"   Final: {best_scores[2]:.5f}")
    print("=" * 60)
    
    # ä¿å­˜æäº¤æ–‡ä»¶
    submission = pd.DataFrame({
        'row_id': range(len(test)),
        'Target': best_pred
    })
    
    output_path = 'submissions/optimized_solution.csv'
    submission.to_csv(output_path, index=False)
    print(f"\nğŸ’¾ å·²ä¿å­˜: {output_path}")
    
    # ç‰¹å¾é‡è¦æ€§
    print("\nğŸ“Š LightGBM ç‰¹å¾é‡è¦æ€§ (Top 20):")
    importance = pd.DataFrame({
        'feature': selected_features,
        'importance': lgb_model.feature_importance()
    }).sort_values('importance', ascending=False)
    
    for i, row in importance.head(20).iterrows():
        print(f"  {row['feature']:<30} {row['importance']:>6.0f}")
    
    return best_pred, y_true_test


if __name__ == '__main__':
    main()
